{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# 设置 Reddit API 凭证\n",
    "reddit = praw.Reddit(\n",
    "    client_id='Gd_KhZrVZVO79ou3LG_vVw',\n",
    "    client_secret='-X_P63zpKewLKEreQJWXm2fFXjY1Ew',\n",
    "    user_agent='reddit-crawler by /u/alpaca_1'\n",
    ")\n",
    "\n",
    "# === 参数设置 ===\n",
    "subreddit_name = \"dogecoin\"\n",
    "query = \"dogecoin\"\n",
    "target_date = datetime(2025, 4, 23)  # 替换为你想要的日期\n",
    "next_date = target_date + timedelta(days=1)\n",
    "\n",
    "# 将目标时间转换为 UTC 时间戳\n",
    "start_timestamp = int(target_date.timestamp())\n",
    "end_timestamp = int(next_date.timestamp())\n",
    "\n",
    "# === 收集帖子 ID ===\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "post_ids = []\n",
    "\n",
    "# 使用 subreddit.search + 时间限制进行搜索\n",
    "for submission in subreddit.search(query, sort=\"new\", time_filter=\"all\", limit=None):\n",
    "    if start_timestamp <= submission.created_utc < end_timestamp:\n",
    "        post_ids.append(submission.id)\n",
    "\n",
    "print(f\"🔍 找到 {len(post_ids)} 篇 {target_date.date()} 当天的包含 'dogecoin' 的帖子\")\n",
    "\n",
    "# === 抓取评论并写入 DataFrame ===\n",
    "rows = []\n",
    "\n",
    "for pid in post_ids:\n",
    "    try:\n",
    "        submission = reddit.submission(id=pid)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            rows.append({\n",
    "                \"post_id\": pid,\n",
    "                \"author\": str(comment.author) if comment.author else \"N/A\",\n",
    "                \"body\": comment.body.replace(\"\\n\", \" \"),\n",
    "                \"score\": comment.score,\n",
    "                \"created_utc\": comment.created_utc,\n",
    "                \"date\": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "        time.sleep(1)  # 防止被限速\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching comments for post {pid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# === 保存为 CSV ===\n",
    "df = pd.DataFrame(rows, columns=[\"post_id\", \"author\", \"body\", \"score\", \"created_utc\", \"date\"])\n",
    "output_filename = f\"dogecoin_comments_{target_date.date()}.csv\"\n",
    "df.to_csv(output_filename, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ 已保存评论至 {output_filename}，共 {len(df)} 条\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 输出文件夹\n",
    "os.makedirs(\"Dogecoin_Comments_HF\", exist_ok=True)\n",
    "output_path = \"Dogecoin_Comments_HF/dogecoin_comments_pushshift.csv\"\n",
    "\n",
    "# 准备流式加载 Pushshift Reddit 评论数据\n",
    "print(\"🔄 开始加载 Hugging Face 上的 Reddit 评论数据（pushshift）...\")\n",
    "\n",
    "# 加载 Pushshift Reddit 评论数据集\n",
    "dataset = load_dataset(\"fddemarco/pushshift-reddit-comments\", split=\"train\", streaming=True)\n",
    "\n",
    "\n",
    "# 筛选 r/dogecoin 的评论并存储\n",
    "comments = []\n",
    "max_comments = 50000  # 你可以改成 100000 或更多，根据需要\n",
    "print(f\"🔍 正在筛选 r/dogecoin 的前 {max_comments} 条评论...\")\n",
    "\n",
    "for i, comment in enumerate(dataset):\n",
    "    if comment.get(\"subreddit\", \"\").lower() == \"dogecoin\":\n",
    "        comments.append({\n",
    "            \"post_id\": comment.get(\"link_id\", \"\").split(\"_\")[-1],\n",
    "            \"author\": comment.get(\"author\", \"N/A\"),\n",
    "            \"body\": comment.get(\"body\", \"\").replace(\"\\n\", \" \"),\n",
    "            \"score\": comment.get(\"score\", 0),\n",
    "            \"created_utc\": comment.get(\"created_utc\"),\n",
    "            \"date\": datetime.utcfromtimestamp(comment[\"created_utc\"]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        print(f\"📥 已抓取 {i} 条，dogecoin 评论数量：{len(comments)}\")\n",
    "    if len(comments) >= max_comments:\n",
    "        break\n",
    "\n",
    "# 转换为 DataFrame 并保存\n",
    "df = pd.DataFrame(comments, columns=[\"post_id\", \"author\", \"body\", \"score\", \"created_utc\", \"date\"])\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ 成功保存 {len(df)} 条评论至 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/Projects/myenvs/reddit/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 正在加载数据集（Hugging Face - fddemarco/pushshift-reddit-comments）...\n",
      "🔍 正在筛选 r/dogecoin 的评论...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 配置\n",
    "SUBREDDIT_NAME = \"dogecoin\"\n",
    "MAX_COMMENTS = 10000  # 可根据内存和需要调整\n",
    "OUTPUT_DIR = \"Dogecoin_Comments_HF\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"{SUBREDDIT_NAME}_comments_sample.csv\")\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 加载 Pushshift Reddit 评论数据（使用 fddemarco 版本）\n",
    "print(\"🔄 正在加载数据集（Hugging Face - fddemarco/pushshift-reddit-comments）...\")\n",
    "dataset = load_dataset(\"fddemarco/pushshift-reddit-comments\", split=\"train\", streaming=True)\n",
    "\n",
    "# 过滤 r/dogecoin 的评论\n",
    "print(f\"🔍 正在筛选 r/{SUBREDDIT_NAME} 的评论...\")\n",
    "filtered = dataset.filter(lambda x: x.get(\"subreddit\") == SUBREDDIT_NAME)\n",
    "\n",
    "# 收集数据\n",
    "comments = []\n",
    "for i, row in enumerate(filtered):\n",
    "    try:\n",
    "        comments.append({\n",
    "            \"post_id\": row.get(\"link_id\", \"\").split(\"_\")[-1],\n",
    "            \"author\": row.get(\"author\", \"N/A\"),\n",
    "            \"body\": row.get(\"body\", \"\").replace(\"\\n\", \" \"),\n",
    "            \"score\": row.get(\"score\", 0),\n",
    "            \"created_utc\": row.get(\"created_utc\"),\n",
    "            \"date\": pd.to_datetime(row.get(\"created_utc\"), unit='s', utc=True).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 跳过错误数据：{e}\")\n",
    "        continue\n",
    "\n",
    "    if i + 1 >= MAX_COMMENTS:\n",
    "        break\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"📥 已抓取 {i + 1} 条评论...\")\n",
    "\n",
    "# 保存为 CSV\n",
    "df = pd.DataFrame(comments, columns=[\"post_id\", \"author\", \"body\", \"score\", \"created_utc\", \"date\"])\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ 已保存 {len(df)} 条评论至 {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
